{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 4\n",
    "## Answered to all questions (1-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "<span style=\"font-size:18px\">Logistic loss function:</span>\n",
    "$$\n",
    "l({\\bf w})=\\sum_{n=0}^{N-1}ln(1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}}))\n",
    "$$\n",
    "<span style=\"font-size:18px\">Gradient of $l({\\bf w})$:</span>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l'({\\bf w})&=\\frac{\\partial}{\\partial{\\bf w}}\\sum_{n=0}^{N-1}ln(1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})) \\\\\n",
    "&= \\sum_{n=0}^{N-1}\\frac{\\partial}{\\partial{\\bf w}}ln(1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})) \\\\\n",
    "\\end{aligned}\n",
    "\\tag{1}\n",
    "\\\\ \\\\\n",
    "$$ \n",
    "    \n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial{\\bf w}}ln(1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}}))&=\\frac{1}{1+exp(-y_{n}{\\bf w^{T}}{\\bf x_{n}})} * \\frac{\\partial}{\\partial{\\bf w}}1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}}) \\\\\n",
    "&=\\frac{1}{1+exp(-y_{n}{\\bf w^{T}}{\\bf x_{n}})} * \\frac{\\partial}{\\partial{\\bf w}}exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})\n",
    "\\end{aligned}\n",
    "\\tag{2}\n",
    "\\\\ \\\\\n",
    "$$\n",
    "    \n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial{\\bf w}}exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})&=exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})*\\frac{\\partial}{\\partial{\\bf w}} -y_{n}{\\bf w^{T}} {\\bf x_{n}}\\\\\n",
    "\\end{aligned}\n",
    "\\tag{3}\n",
    "$$\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial{\\bf w}}-y_{n}{\\bf w^{T}} {\\bf x_{n}}&=-y_{n}\\frac{\\partial}{\\partial{\\bf w}}{\\bf w^{T}} {\\bf x_{n}} \\\\\n",
    "&=-y_{n}{\\bf x_{n}}\n",
    "\\end{aligned}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "<span style=\"font-size:18px\">Therefore:</span>\n",
    "$$\n",
    "(1),(2),(3),(4)\\Rightarrow l'({\\bf w})=\\sum_{n=0}^{N-1}\\frac{exp(-y_{n}{\\bf w^{T}}{\\bf x_{n}})}{1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})}(-y_{n}{\\bf x_{n}})\n",
    "\\\\\\\\\n",
    "$$\n",
    "    \n",
    "<span style=\"font-size:18px\">Gradient of $C*{\\bf w^{T}}{\\bf w}$:</span>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial{\\bf w}}C*{\\bf w^{T}}{\\bf w}\n",
    "&=C*\\frac{\\partial}{\\partial{\\bf w}}{\\bf w^{T}}{\\bf w} \\\\\n",
    "&=C*\\frac{\\partial}{\\partial{\\bf w}}\\sum w_{i}^{2} \\\\\n",
    "&=C*(2w_{0}, \\cdots, 2w_{P})^{T} \\\\\n",
    "&=2C{\\bf w} \\\\\n",
    "\\end{aligned}\n",
    "\\\\\n",
    "$$\n",
    "    \n",
    "<span style=\"font-size:18px\">Therefore, the gradient of the *L<sub>2</sub>-regularized logistic loss* is:</span>   \n",
    "    \n",
    "$$\n",
    "l'({\\bf w}) = \\sum_{n=0}^{N-1}\\frac{exp(-y_{n}{\\bf w^{T}}{\\bf x_{n}})}{1+exp(-y_{n}{\\bf w^{T}} {\\bf x_{n}})}(-y_{n}{\\bf x_{n}}) + 2C{\\bf w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">a)</span>\n",
    "![alt text](Q2a.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">b) Number of parameters in each layer</span>\n",
    "\n",
    "Input layer = 0\n",
    "    \n",
    "    No learnable parameters.\n",
    "    \n",
    "First convolution = $5*5*3*10+10 = 760$\n",
    "    \n",
    "    Convolution window (weight matrix) of size 5x5. Therefore 5*5 learnable parameters.\n",
    "    \n",
    "    Need to learn a unique conv. window for each feature map in the input. Therefore *3.\n",
    "    \n",
    "    Need to learn a unique conv. window parameters for each feature map in the output. Therefore *10.\n",
    "    \n",
    "    Bias for each feature map in output. Therefore +10.\n",
    "    \n",
    "    \n",
    "Max-Pool = 0\n",
    "    \n",
    "    No learnable parameters. Just shrinks the size of each feature map.\n",
    "    \n",
    "Second convolution = $5*5*10*10+10=2510$\n",
    "    \n",
    "    Convolution window (weight matrix) of size 5x5. Therefore 5*5 learnable parameters.\n",
    "    \n",
    "    Need to learn a unique conv. window for each feature map in the input. Therefore *10.\n",
    "    \n",
    "    Need to learn a unique conv. window parameters for each feature map in the output. Therefore *10.\n",
    "    \n",
    "    Bias for each feature map in output. Therefore +10.\n",
    "    \n",
    "Max-Pool = 0\n",
    "    \n",
    "Flatten = 0\n",
    "    \n",
    "    No learnable parameters. Just reduces the dimensionality of the input.\n",
    "    \n",
    "Output layer = $2560*2+2 = 5122$\n",
    "    \n",
    "    2560 neurons in previous layer connected to output layer of size of 2. Therefore 2560*2 weight parameters.\n",
    "    \n",
    "    Also, the neurons in the output layer have bias term each. Therefore +2 parameters.\n",
    "    \n",
    "Total number of parameters in network = 8392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# Load the data\n",
    "os.chdir('/home/tuomas/Python/DATA.ML.200/Ex4')\n",
    "dataX = np.loadtxt('X.csv', delimiter=',')\n",
    "dataY = np.loadtxt('y.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1. w = [-1.40993347  1.51911066], log-loss = 120.0308246119129\n",
      "Iteration 2. w = [-0.97233901  1.62664125], log-loss = 100.6582735812171\n",
      "Iteration 3. w = [-0.64919303  1.6856105 ], log-loss = 91.0521173245905\n",
      "Iteration 4. w = [-0.40953901  1.72960377], log-loss = 85.81339724422338\n",
      "Iteration 5. w = [-0.2272205   1.76830622], log-loss = 82.54460203484516\n",
      "Iteration 6. w = [-0.08380088  1.80327307], log-loss = 80.25616945229795\n",
      "Iteration 7. w = [0.03270673 1.83460917], log-loss = 78.53482482724279\n",
      "Iteration 8. w = [0.12982235 1.86261584], log-loss = 77.19063349610738\n",
      "Iteration 9. w = [0.21229145 1.88783656], log-loss = 76.12211418603785\n",
      "Iteration 10. w = [0.28322348 1.91085883], log-loss = 75.26596446102715\n",
      "Iteration 11. w = [0.34477385 1.93218611], log-loss = 74.57762982649841\n",
      "Iteration 12. w = [0.39852951 1.95219344], log-loss = 74.02328617845696\n",
      "Iteration 13. w = [0.44572056 1.97113183], log-loss = 73.57624251642194\n",
      "Iteration 14. w = [0.48733564 1.98915342], log-loss = 73.21511706889832\n",
      "Iteration 15. w = [0.52418667 2.00634089], log-loss = 72.92273788286835\n",
      "Iteration 16. w = [0.55694791 2.02273332], log-loss = 72.6853525537232\n",
      "Iteration 17. w = [0.58618244 2.03834596], log-loss = 72.4919922323916\n",
      "Iteration 18. w = [0.61236196 2.05318369], log-loss = 72.33393785996226\n",
      "Iteration 19. w = [0.63588265 2.0672494 ], log-loss = 72.20427191194483\n",
      "Iteration 20. w = [0.65707838 2.08054867], log-loss = 72.09750761745924\n",
      "Iteration 21. w = [0.67623158 2.0930918 ], log-loss = 72.00928774019741\n",
      "Iteration 22. w = [0.69358228 2.10489443], log-loss = 71.93614369723193\n",
      "Iteration 23. w = [0.70933553 2.11597715], log-loss = 71.87530522180367\n",
      "Iteration 24. w = [0.72366743 2.12636473], log-loss = 71.82455117990301\n",
      "Iteration 25. w = [0.73673002 2.13608519], log-loss = 71.78209319253463\n",
      "Iteration 26. w = [0.74865529 2.14516879], log-loss = 71.74648502385945\n",
      "Iteration 27. w = [0.75955841 2.15364726], log-loss = 71.71655201145197\n",
      "Iteration 28. w = [0.76954031 2.16155299], log-loss = 71.69133599956866\n",
      "Iteration 29. w = [0.77868989 2.16891844], log-loss = 71.67005223514175\n",
      "Iteration 30. w = [0.78708574 2.17577568], log-loss = 71.65205549424364\n",
      "Iteration 31. w = [0.79479763 2.18215603], log-loss = 71.6368133431265\n",
      "Iteration 32. w = [0.80188769 2.18808975], log-loss = 71.6238849304589\n",
      "Iteration 33. w = [0.80841144 2.19360589], log-loss = 71.61290408447032\n",
      "Iteration 34. w = [0.81441865 2.19873217], log-loss = 71.60356577563259\n",
      "Iteration 35. w = [0.81995402 2.20349487], log-loss = 71.59561522321077\n",
      "Iteration 36. w = [0.82505784 2.20791885], log-loss = 71.58883908914257\n",
      "Iteration 37. w = [0.82976648 2.2120275 ], log-loss = 71.58305832814172\n",
      "Iteration 38. w = [0.83411284 2.21584281], log-loss = 71.57812235847256\n",
      "Iteration 39. w = [0.83812675 2.21938536], log-loss = 71.57390429089816\n",
      "Iteration 40. w = [0.84183532 2.22267441], log-loss = 71.57029700941074\n",
      "Iteration 41. w = [0.84526319 2.22572794], log-loss = 71.56720994064261\n",
      "Iteration 42. w = [0.8484328  2.22856271], log-loss = 71.56456638242473\n",
      "Iteration 43. w = [0.85136466 2.23119436], log-loss = 71.56230128812578\n",
      "Iteration 44. w = [0.85407746 2.2336374 ], log-loss = 71.56035942390605\n",
      "Iteration 45. w = [0.85658834 2.23590536], log-loss = 71.55869383215281\n",
      "Iteration 46. w = [0.85891296 2.23801081], log-loss = 71.55726454714234\n",
      "Iteration 47. w = [0.86106568 2.23996541], log-loss = 71.55603751911839\n",
      "Iteration 48. w = [0.8630597  2.24178001], log-loss = 71.55498371108892\n",
      "Iteration 49. w = [0.86490711 2.24346468], log-loss = 71.55407833914339\n",
      "Iteration 50. w = [0.86661905 2.24502875], log-loss = 71.55330023233239\n",
      "Iteration 51. w = [0.86820573 2.2464809 ], log-loss = 71.55263129238745\n",
      "Iteration 52. w = [0.86967658 2.24782918], log-loss = 71.55205603699386\n",
      "Iteration 53. w = [0.87104028 2.24908105], log-loss = 71.55156121313833\n",
      "Iteration 54. w = [0.87230482 2.25024343], log-loss = 71.55113546933262\n",
      "Iteration 55. w = [0.87347756 2.25132277], log-loss = 71.5507690774014\n",
      "Iteration 56. w = [0.87456532 2.25232501], log-loss = 71.550453696061\n",
      "Iteration 57. w = [0.87557437 2.25325569], log-loss = 71.55018216979416\n",
      "Iteration 58. w = [0.87651051 2.25411995], log-loss = 71.54994835757547\n",
      "Iteration 59. w = [0.87737909 2.25492254], log-loss = 71.54974698688189\n",
      "Iteration 60. w = [0.87818506 2.25566788], log-loss = 71.54957352914379\n",
      "Iteration 61. w = [0.87893301 2.25636008], log-loss = 71.5494240934032\n",
      "Iteration 62. w = [0.87962717 2.25700294], log-loss = 71.54929533544599\n",
      "Iteration 63. w = [0.88027145 2.25759998], log-loss = 71.54918438010375\n",
      "Iteration 64. w = [0.88086948 2.25815449], log-loss = 71.54908875477132\n",
      "Iteration 65. w = [0.88142462 2.25866951], log-loss = 71.54900633248657\n",
      "Iteration 66. w = [0.88193997 2.25914785], log-loss = 71.5489352831695\n",
      "Iteration 67. w = [0.88241841 2.25959214], log-loss = 71.54887403182899\n",
      "Iteration 68. w = [0.88286261 2.26000481], log-loss = 71.54882122272359\n",
      "Iteration 69. w = [0.88327503 2.26038811], log-loss = 71.54877568861369\n",
      "Iteration 70. w = [0.88365798 2.26074415], log-loss = 71.54873642437141\n",
      "Iteration 71. w = [0.88401356 2.26107486], log-loss = 71.54870256432105\n",
      "Iteration 72. w = [0.88434375 2.26138204], log-loss = 71.54867336277557\n",
      "Iteration 73. w = [0.88465038 2.26166739], log-loss = 71.54864817731249\n",
      "Iteration 74. w = [0.88493512 2.26193244], log-loss = 71.5486264543998\n",
      "Iteration 75. w = [0.88519956 2.26217866], log-loss = 71.54860771703694\n",
      "Iteration 76. w = [0.88544515 2.26240737], log-loss = 71.54859155412592\n",
      "Iteration 77. w = [0.88567323 2.26261983], log-loss = 71.5485776113283\n",
      "Iteration 78. w = [0.88588507 2.2628172 ], log-loss = 71.54856558319858\n",
      "Iteration 79. w = [0.88608182 2.26300053], log-loss = 71.54855520641219\n",
      "Iteration 80. w = [0.88626455 2.26317085], log-loss = 71.5485462539389\n",
      "Iteration 81. w = [0.88643428 2.26332906], log-loss = 71.5485385300238\n",
      "Iteration 82. w = [0.88659193 2.26347604], log-loss = 71.54853186586675\n",
      "Iteration 83. w = [0.88673837 2.26361258], log-loss = 71.54852611590074\n",
      "Iteration 84. w = [0.88687438 2.26373942], log-loss = 71.5485211545856\n",
      "Iteration 85. w = [0.88700072 2.26385725], log-loss = 71.54851687364747\n",
      "Iteration 86. w = [0.88711808 2.26396671], log-loss = 71.54851317969764\n",
      "Iteration 87. w = [0.88722709 2.26406841], log-loss = 71.54850999218289\n",
      "Iteration 88. w = [0.88732836 2.26416288], log-loss = 71.5485072416178\n",
      "Iteration 89. w = [0.88742242 2.26425064], log-loss = 71.5485048680613\n",
      "Iteration 90. w = [0.8875098  2.26433218], log-loss = 71.5485028198036\n",
      "Iteration 91. w = [0.88759098 2.26440792], log-loss = 71.54850105223437\n",
      "Iteration 92. w = [0.88766638 2.26447829], log-loss = 71.54849952686651\n",
      "Iteration 93. w = [0.88773643 2.26454366], log-loss = 71.5484982104944\n",
      "Iteration 94. w = [0.8878015 2.2646044], log-loss = 71.5484970744685\n",
      "Iteration 95. w = [0.88786194 2.26466082], log-loss = 71.54849609406955\n",
      "Iteration 96. w = [0.8879181  2.26471324], log-loss = 71.54849524796849\n",
      "Iteration 97. w = [0.88797027 2.26476193], log-loss = 71.54849451776158\n",
      "Iteration 98. w = [0.88801873 2.26480718], log-loss = 71.54849388756827\n",
      "Iteration 99. w = [0.88806375 2.26484921], log-loss = 71.54849334368532\n",
      "Iteration 100. w = [0.88810557 2.26488825], log-loss = 71.54849287428814\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "\n",
    "def add_bias(w, X, b):\n",
    "    new_w = w.tolist()\n",
    "    new_w.append(b)\n",
    "    new_X = np.ones((X.shape[0], X.shape[1]+1))\n",
    "    new_X[:,:-1] = X\n",
    "    return np.array(new_w), new_X\n",
    "\n",
    "def log_loss(w):\n",
    "    loss = 0\n",
    "    for n in range(X.shape[0]):\n",
    "        x = X[n]\n",
    "        y = dataY[n]\n",
    "        loss += np.log(1 + np.exp(-y * w@x))\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def loss_gradient(w):\n",
    "    grad = np.zeros(w.shape)\n",
    "    for n in range(X.shape[0]):\n",
    "        x = X[n]\n",
    "        y = dataY[n]\n",
    "        num = np.exp(-y * w@x)\n",
    "        denom = 1 + np.exp(-y * w@x)\n",
    "        grad += (num/denom)*(-y*x)\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def predict(w):\n",
    "    preds = []\n",
    "    for n in range(X.shape[0]):\n",
    "        x = X[n]\n",
    "        class_1 = (1 / (1 + np.exp(-w@x))) > 0.5\n",
    "        if class_1:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(-1)\n",
    "            \n",
    "    return np.array(preds)\n",
    "\n",
    "# Initialize w as random\n",
    "w = np.random.randn(2)\n",
    "# Add bias term to the model (init. as random as well)\n",
    "w, X = add_bias(w, dataX, np.random.randn(1)[0])\n",
    "\n",
    "# Set learning rate\n",
    "e = 0.01\n",
    "iterations = 100\n",
    "old_ws = []\n",
    "old_accs = []\n",
    "for i in range(iterations):\n",
    "    # Update w\n",
    "    w = w - e*loss_gradient(w)\n",
    "    # Print info\n",
    "    print('Iteration {}. w = {}, log-loss = {}'.format(i+1, w[:2], log_loss(w)))\n",
    "    # Calculate accuracy\n",
    "    y_hat = predict(w)\n",
    "    acc = np.sum(y_hat == dataY) / 400.0\n",
    "    old_accs.append(acc)\n",
    "    old_ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "old_ws = np.array(old_ws)\n",
    "old_accs = np.array(old_accs)\n",
    "\n",
    "fig = plt.figure(figsize = [6,6])\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(old_ws[:,0], old_ws[:,1], 'ro-')\n",
    "blue_dot = ax1.scatter(old_ws[0,0], old_ws[0,1], color='blue', zorder=10)\n",
    "green_dot = ax1.scatter(old_ws[-1,0], old_ws[-1,1], color='green', zorder=10)\n",
    "plt.legend([blue_dot, green_dot], ['starting point','ending point'])\n",
    "plt.grid()\n",
    "plt.xlabel('w$_0$')\n",
    "plt.ylabel('w$_1$')\n",
    "plt.title('Optimization path')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(100.0 * old_accs, linewidth = 2)\n",
    "plt.grid()\n",
    "plt.ylabel('Accuracy / %')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Accuracy evolution')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "<span style=\"font-size:18px\">Answered to question b)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 80,273\n",
      "Trainable params: 80,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "N = 32 # Number of feature maps\n",
    "w, h = 5, 5 # Conv. window size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(N, (w, h),input_shape=(64, 64, 3),activation = 'relu',padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Conv2D(N, (w, h),activation = 'relu',padding = 'same'))\n",
    "model.add(MaxPooling2D((4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation = 'sigmoid'))\n",
    "model.add(Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000/*.jpg\n",
      "00001/*.jpg\n",
      "00002/*.jpg\n",
      "00003/*.jpg\n",
      "00004/*.jpg\n",
      "00005/*.jpg\n",
      "00006/*.jpg\n",
      "00007/*.jpg\n",
      "00008/*.jpg\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imread_collection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "# Compile the network of Question 4 b\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Load the data\n",
    "os.chdir('/home/tuomas/Python/DATA.ML.200/Ex3')\n",
    "images = []\n",
    "labels = []\n",
    "for i in range(0,9):\n",
    "    fn = '0000{}/*.jpg'.format(i)\n",
    "    print(fn)\n",
    "    imgs = imread_collection(fn)\n",
    "    images.append(np.array(imgs, dtype='object'))\n",
    "    labels.append( np.ones(len(imgs)) * i )\n",
    "\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all images to 64 x 64\n",
    "images_resized = []\n",
    "for img in images:\n",
    "    img_r = cv2.resize(img, (64,64))\n",
    "    images_resized.append(img_r/255.)\n",
    "\n",
    "images_resized = np.array(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "trainX, testX, trainY, testY = train_test_split(images_resized, \n",
    "                                                labels, \n",
    "                                                test_size=0.15)\n",
    "\n",
    "# One-hot encoding\n",
    "trainY_cat = to_categorical(trainY, num_classes=9)\n",
    "testY_cat = to_categorical(testY, num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 1.8738 - accuracy: 0.2727 - val_loss: 1.5935 - val_accuracy: 0.4256\n",
      "Epoch 2/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 1.3079 - accuracy: 0.5489 - val_loss: 0.9500 - val_accuracy: 0.7210\n",
      "Epoch 3/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.7369 - accuracy: 0.7832 - val_loss: 0.5339 - val_accuracy: 0.8504\n",
      "Epoch 4/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.4071 - accuracy: 0.8928 - val_loss: 0.3252 - val_accuracy: 0.9155\n",
      "Epoch 5/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.2296 - accuracy: 0.9519 - val_loss: 0.2471 - val_accuracy: 0.9342\n",
      "Epoch 6/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.1483 - accuracy: 0.9690 - val_loss: 0.1675 - val_accuracy: 0.9604\n",
      "Epoch 7/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.1018 - accuracy: 0.9819 - val_loss: 0.1215 - val_accuracy: 0.9678\n",
      "Epoch 8/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0691 - accuracy: 0.9886 - val_loss: 0.0948 - val_accuracy: 0.9723\n",
      "Epoch 9/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0439 - accuracy: 0.9963 - val_loss: 0.0811 - val_accuracy: 0.9798\n",
      "Epoch 10/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0342 - accuracy: 0.9967 - val_loss: 0.0707 - val_accuracy: 0.9828\n",
      "Epoch 11/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0228 - accuracy: 0.9987 - val_loss: 0.0669 - val_accuracy: 0.9791\n",
      "Epoch 12/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0185 - accuracy: 0.9991 - val_loss: 0.0664 - val_accuracy: 0.9813\n",
      "Epoch 13/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0293 - accuracy: 0.9955 - val_loss: 0.0688 - val_accuracy: 0.9776\n",
      "Epoch 14/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0100 - accuracy: 0.9997 - val_loss: 0.0473 - val_accuracy: 0.9843\n",
      "Epoch 15/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0129 - accuracy: 0.9989 - val_loss: 0.0790 - val_accuracy: 0.9768\n",
      "Epoch 16/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0139 - accuracy: 0.9984 - val_loss: 0.0565 - val_accuracy: 0.9843\n",
      "Epoch 17/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0065 - accuracy: 0.9997 - val_loss: 0.0435 - val_accuracy: 0.9865\n",
      "Epoch 18/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9895\n",
      "Epoch 19/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9873\n",
      "Epoch 20/20\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9888\n",
      "42/42 - 0s - loss: 0.0343 - accuracy: 0.9888\n",
      "Accuracy of CNN = 0.9887808561325073\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "# Train the model\n",
    "history = model.fit(trainX, trainY_cat, batch_size=32, epochs=20, validation_data=(testX, testY_cat), verbose=1)\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(testX, testY_cat, verbose=2)\n",
    "print(\"Accuracy of CNN = {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
